{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp cnn_oc_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cnn_oc_svm\n",
    "\n",
    "> Contains the base class for the CNN OC-SVM model. Override the SVM or CNN as desired but used directly is optimized for the MNIST example case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class cnn_oc_svm:\n",
    "    \"\"\"\n",
    "    The base-class with simple CNN and OC-SVM.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic neural net for mnist digits\n",
    "Let's build a quick neural net in pytorch to classify MNIST digits. I'm assuming if you're this far deep into machine learning, you likely have encountered this before. If you are unsure about the basics of Neural Networks as classifiers, you should check out an in depth tutorial for the nitty-gritty details. We're doing this in a standard but terse approach.\n",
    "\n",
    "### The general idea:\n",
    "\n",
    "In training a neural network we are transforming images into feature maps and feeding that into the final fully connected layer that corresponds to our classes. Once we have a model with the performance we like, we separate the final layer from the rest of the neural net. We then use this feature extractor on our training set to generate our training data for the one-class SVM.\n",
    "\n",
    "Why do we care? Because then we can do this on incoming data and check if the incoming image data is anomalous. Depending on your use case this can mean auto-flagging useful and novel future training data or could mean flagging potentially fraudulent or dangerous input.\n",
    "\n",
    "This will be demonstrated with an MNIST classifier with the hope to have the OC-SVM flag input images that are not numbers but random images, patterns, and possibly even letters.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. Load up MNIST dataset and setup data pipeline\n",
    "2. Construct CNN in pytorch and train\n",
    "3. Freeze model and separate the last fully connected layer from model\n",
    "    * Ideally we can pass the vector straight to the last layer\n",
    "4. Use the feature extractor to generate data for our OC-SVM model\n",
    "\n",
    "Once that is complete we'll have the first component of our lovely model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Creating the data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.5%5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Loading up our transformers so images get preprocessed\n",
    "# TODO Add robust preprocessing so it'll work out of the box for more realistic problems\n",
    "image_preprocessing_pipeline = transforms.Compose([\n",
    "                               transforms.ToTensor(), #Convert image to tensor\n",
    "                               transforms.Normalize((0.5,), (0.5,)), # Normalize RGB values from 0-255 to 0-1\n",
    "                               ])\n",
    "\n",
    "# Building dataset\n",
    "training_data = datasets.MNIST('.', train=True, transform=image_preprocessing_pipeline, download=True)\n",
    "\n",
    "validation_data = datasets.MNIST('.', train=False, transform=image_preprocessing_pipeline, download=True)\n",
    "\n",
    "# Setting up data loaders.\n",
    "training_data_loader = torch.utils.data.DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "\n",
    "validation_data_loader = torch.utils.data.DataLoader(validation_data, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28 * 28 #Image resolution is 28x28\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_sizes[0]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[1], output_size),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "\n",
    "criterion = nn.NLLLoss() #Negative log-likelihood loss cuz classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick detour for looking at our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note the shape of a single image: \n",
      " torch.Size([1, 28, 28])\n",
      "Post numpy and squeeze gives us a 28x28 matrix representation of our image with shape: \n",
      " (28, 28)\n",
      "this example image is a 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(training_data_loader))\n",
    "print(f\"Note the shape of a single image: \\n {images[0].shape}\")\n",
    "example_image = images[0].numpy().squeeze()\n",
    "print(f\"Post numpy and squeeze gives us a 28x28 matrix representation of our image with shape: \\n {example_image.shape}\")\n",
    "plt.imshow(example_image, cmap='gray_r')\n",
    "print(f\"this example image is a {labels[0].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backwards pass model weights: \n",
      " None\n",
      "After backwards pass: \n",
      " tensor([[-0.0019, -0.0019, -0.0019,  ..., -0.0019, -0.0019, -0.0019],\n",
      "        [ 0.0025,  0.0025,  0.0025,  ...,  0.0025,  0.0025,  0.0025],\n",
      "        [ 0.0014,  0.0014,  0.0014,  ...,  0.0014,  0.0014,  0.0014],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0026,  0.0026,  0.0026,  ...,  0.0026,  0.0026,  0.0026],\n",
      "        [-0.0017, -0.0017, -0.0017,  ..., -0.0017, -0.0017, -0.0017]])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(training_data_loader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logps = model(images)\n",
    "loss = criterion(logps, labels)\n",
    "\n",
    "print(f\"Before backwards pass model weights: \\n {model[0].weight.grad}\")\n",
    "loss.backward()\n",
    "print(f\"After backwards pass: \\n {model[0].weight.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.11525126427475578\n",
      "Training time (s): 13.837715148925781\n",
      "Epoch 1 - Training loss: 0.1025592965145371\n",
      "Training time (s): 30.546591997146606\n",
      "Epoch 2 - Training loss: 0.0905685454030146\n",
      "Training time (s): 46.87804174423218\n",
      "Epoch 3 - Training loss: 0.08280672240150429\n",
      "Training time (s): 63.4960994720459\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in training_data_loader:\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predicted_labels = model(images)\n",
    "        loss = criterion(predicted_labels, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        print(f\"Epoch {epoch} - Training loss: {running_loss/len(training_data_loader)}\")\n",
    "    \n",
    "    print(f\"Training time (s): {time.time()-start_time}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images tested: 10000 \n",
      " Classification Accuracy: 0.9702\n"
     ]
    }
   ],
   "source": [
    "correct_classification, total_images = 0, 0\n",
    "\n",
    "for images, labels in validation_data_loader:\n",
    "    for i in range(len(labels)):\n",
    "        img = images[i].view(1, 784)\n",
    "        with torch.no_grad():\n",
    "            logps = model(img)\n",
    "        \n",
    "        ps = torch.exp(logps)\n",
    "        probab = list(ps.numpy()[0])\n",
    "        \n",
    "        pred_label = probab.index(max(probab))\n",
    "        true_label = labels.numpy()[i]\n",
    "        \n",
    "        if pred_label == true_label:\n",
    "            correct_classification += 1\n",
    "        total_images += 1\n",
    "        \n",
    "print(f\"Images tested: {total_images} \\n Classification Accuracy: {correct_classification/total_images}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_model = model[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_model = testing_model.eval()\n",
    "final_output_layer = model[-2:].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(example_image)\n",
    "\n",
    "feature_vector = frozen_model(torch.tensor(example_image).view(1, -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5194e+01, -1.5312e+01, -1.1790e+01, -3.0279e-05, -2.6338e+01,\n",
       "         -1.1346e+01, -2.7744e+01, -1.2008e+01, -1.4417e+01, -1.2481e+01]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output_layer(feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_result = model.eval()(torch.tensor(example_image).view(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5194e+01, -1.5312e+01, -1.1790e+01, -3.0279e-05, -2.6338e+01,\n",
       "         -1.1346e+01, -2.7744e+01, -1.2008e+01, -1.4417e+01, -1.2481e+01]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
