{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp cnn_oc_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cnn_oc_svm\n",
    "\n",
    "> Contains the base class for the CNN OC-SVM model. Override the SVM or CNN as desired but used directly is optimized for the MNIST example case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "class cnn_oc_svm:\n",
    "    \"\"\"\n",
    "    The base-class for building a neural network with oc-SVM.\n",
    "    \"\"\"\n",
    "    def __init__(self, neural_net, criterion, optimizer):\n",
    "        self.neural_net = neural_net\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.oc_svm = None\n",
    "    def training(self, epochs, learning_rate, training_data_loader, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Basic training function for nn.Sequence style models.\n",
    "        \"\"\"\n",
    "\n",
    "        for epoch in range(1, epochs+1):\n",
    "            running_loss = 0\n",
    "            for images, labels in training_data_loader:\n",
    "                images = images.view(images.shape[0], -1)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                predicted_labels = self.neural_net(images)\n",
    "                loss = self.criterion(predicted_labels, labels)\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                self.running_loss += loss.item()\n",
    "\n",
    "            else:\n",
    "                print(f\"Epoch {epoch} - Training loss: {running_loss/len(training_data_loader)}\")\n",
    "\n",
    "        print(f\"Training time (s): {time.time()-start_time}\")\n",
    "        return running_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic neural net for mnist digits\n",
    "Let's build a quick neural net in pytorch to classify MNIST digits. I'm assuming if you're this far deep into machine learning, you likely have encountered this before. If you are unsure about the basics of Neural Networks as classifiers, you should check out an in depth tutorial for the nitty-gritty details. We're doing this in a standard but terse approach.\n",
    "\n",
    "### The general idea:\n",
    "\n",
    "In training a neural network we are transforming images into feature maps and feeding that into the final fully connected layer that corresponds to our classes. Once we have a model with the performance we like, we separate the final layer from the rest of the neural net. We then use this feature extractor on our training set to generate our training data for the one-class SVM.\n",
    "\n",
    "Why do we care? Because then we can do this on incoming data and check if the incoming image data is anomalous. Depending on your use case this can mean auto-flagging useful and novel future training data or could mean flagging potentially fraudulent or dangerous input.\n",
    "\n",
    "This will be demonstrated with an MNIST classifier with the hope to have the OC-SVM flag input images that are not numbers but random images, patterns, and possibly even letters.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. Load up MNIST dataset and setup data pipeline\n",
    "2. Construct CNN in pytorch and train\n",
    "3. Freeze model and separate the last fully connected layer from model\n",
    "    * Ideally we can pass the vector straight to the last layer\n",
    "4. Use the feature extractor to generate data for our OC-SVM model\n",
    "\n",
    "Once that is complete we'll have the first component of our lovely model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Creating the data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading up our transformers so images get preprocessed\n",
    "# TODO Add robust preprocessing so it'll work out of the box for more realistic problems\n",
    "image_preprocessing_pipeline = transforms.Compose([\n",
    "                               transforms.ToTensor(), #Convert image to tensor\n",
    "                               transforms.Normalize((0.5,), (0.5,)), # Normalize RGB values from 0-255 to 0-1\n",
    "                               ])\n",
    "\n",
    "# Building dataset\n",
    "training_data = datasets.MNIST('.', train=True, transform=image_preprocessing_pipeline, download=True)\n",
    "\n",
    "validation_data = datasets.MNIST('.', train=False, transform=image_preprocessing_pipeline, download=True)\n",
    "\n",
    "# Setting up data loaders.\n",
    "training_data_loader = torch.utils.data.DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "\n",
    "validation_data_loader = torch.utils.data.DataLoader(validation_data, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build the Neural Network classifier\n",
    "\n",
    "We build a simple fully connected neural network. Our input layer takes in our 28x28 image and flattens it out. We pass it through an activation function (ReLU) and through a hidden layer before outputting to a final fully connected layer and a softmax activation layer so our image turns into a feature vector and finally class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_basic_neural_net_structure(input_size=28*28, hidden_sizes=[128, 64], number_of_classes=10):\n",
    "    \"\"\"\n",
    "    Creates basic neural network with an input layer, feature extraction layer, and output layer.\n",
    "    Basic structure:\n",
    "        Input: linear layer of image resolution (color * width * height) and ReLU activation\n",
    "        Hidden: linear layer of hidden sizes and RelU activation\n",
    "        Output: liear layer and LogSoftmax activation to final size of number_of_classes\n",
    "    parameters:\n",
    "        input_size: number of neurons in input layer\n",
    "        hidden_sizes: in_feature and out_feature sizes as ints. \n",
    "    returns: Sequential neural net for training and inference.\n",
    "    \"\"\"\n",
    "    if not isinstance(input_size, int):\n",
    "        raise TypeError(f\"Input size must be an integer, received type {type(input_size)}\")\n",
    "    if input_size <= 0:\n",
    "        raise ValueError(f\"Input must be greater than 0 and an integer, received {input_size}\")\n",
    "        \n",
    "    input_layer_dimensions = (input_size, hidden_sizes[0]) if len(hidden_sizes) != 0 else (input_size, number_of_classes)\n",
    "    input_layers = [nn.Linear(*input_layer_dimensions), nn.ReLU()]\n",
    "    \n",
    "    if len(hidden_sizes) == 0:\n",
    "        hidden_layers = list()\n",
    "    else:\n",
    "        hidden_layers = [layer for dims in zip(hidden_sizes[:-1], hidden_sizes[1:]) for layer in (nn.Linear(*dims), nn.ReLU())]\n",
    "    \n",
    "    final_output_layer_dimensions = (hidden_sizes[-1], number_of_classes) if len(hidden_sizes) != 0 else (input_size, number_of_classes)\n",
    "    \n",
    "    final_output_layer = [nn.Linear(*final_output_layer_dimensions), nn.LogSoftmax()]\n",
    "    \n",
    "    model_structure_list = list()\n",
    "    for structure in [input_layers, hidden_layers, final_output_layer]:\n",
    "        model_structure_list.extend(structure)\n",
    "    \n",
    "    model = nn.Sequential(*model_structure_list)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some basic tests on our helper function to generate a fully connected neural network for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_size = 28*28\n",
    "test_hidden_sizes = [128, 64]\n",
    "test_number_of_classes = 10\n",
    "\n",
    "base_mnist_model_structure = nn.Sequential(\n",
    "    nn.Linear(28*28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10),\n",
    "    nn.LogSoftmax()\n",
    ")\n",
    "\n",
    "### Testing of improper input_size parameters\n",
    "for improper_input_size in [0, -10, -1000, None, 293.4, \"foobar\"]:\n",
    "\n",
    "    try:\n",
    "\n",
    "        create_basic_neural_net_structure(input_size=improper_input_size)\n",
    "    except ValueError as good_error:\n",
    "        assert str(ValueError(f\"Input must be greater than 0 and an integer, received {improper_input_size}\")) == str(good_error)\n",
    "    except TypeError as good_error:\n",
    "        assert str(good_error) == str(TypeError(f\"Input size must be an integer, received type {type(improper_input_size)}\"))\n",
    "\n",
    "# This is for testing that valid input sizes (cooresponding to 28x28, 50x50, and 300x300 grayscale image resolutions) create networks as expected        \n",
    "for test_input_size in [28**2, 50**2, 300**2]:\n",
    "    test_mnist_model = create_basic_neural_net_structure(test_input_size, test_hidden_sizes, test_number_of_classes)\n",
    "\n",
    "    base_mnist_model_structure = nn.Sequential(\n",
    "        nn.Linear(test_input_size, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 10),\n",
    "        nn.LogSoftmax()\n",
    "    )\n",
    "    \n",
    "    # This asserts that the models contain the same number of layers\n",
    "    assert len(test_mnist_model) == len(base_mnist_model_structure)\n",
    "\n",
    "    for idx, layers in enumerate(zip(base_mnist_model_structure, test_mnist_model)):\n",
    "        if idx % 2 != 0:\n",
    "            # We skip every 2nd layer as they're the ReLU activation layers.\n",
    "            continue\n",
    "        else:\n",
    "            # Confirming our layers both match in their dimensions\n",
    "            their_layer, our_layer = layers\n",
    "            assert their_layer.in_features == our_layer.in_features\n",
    "            assert their_layer.out_features == our_layer.out_features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick detour for looking at our data\n",
    "\n",
    "Let's take a quick look at the shape, label, and how we transform the data before feeding it to our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note the shape of a single image: \n",
      " torch.Size([1, 28, 28])\n",
      "Post numpy and squeeze gives us a 28x28 matrix representation of our image with shape: \n",
      "    (28, 28)\n",
      "this example image is a 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(training_data_loader))\n",
    "print(f\"Note the shape of a single image: \\n {images[0].shape}\")\n",
    "example_image = images[0].numpy().squeeze()\n",
    "print(f\"Post numpy and squeeze gives us a 28x28 matrix representation of our image with shape: \\n    {example_image.shape}\")\n",
    "plt.imshow(example_image, cmap='gray_r')\n",
    "print(f\"this example image is a {labels[0].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 0.6164425287419545\n",
      "Epoch 2 - Training loss: 0.2789036152618272\n",
      "Epoch 3 - Training loss: 0.21704437942710766\n",
      "Training time (s): 36.0192015171051\n"
     ]
    }
   ],
   "source": [
    "model = create_basic_neural_net_structure()\n",
    "criterion = nn.NLLLoss() #Negative log-likelihood loss which is used because we're doing classification\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    running_loss = 0\n",
    "    for images, labels in training_data_loader:\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predicted_labels = model(images)\n",
    "        loss = criterion(predicted_labels, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        print(f\"Epoch {epoch} - Training loss: {running_loss/len(training_data_loader)}\")\n",
    "    \n",
    "print(f\"Training time (s): {time.time()-start_time}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images tested: 10000 \n",
      " Classification Accuracy: 0.9463\n"
     ]
    }
   ],
   "source": [
    "correct_classification, total_images = 0, 0\n",
    "\n",
    "for images, labels in validation_data_loader:\n",
    "    for i in range(len(labels)):\n",
    "        img = images[i].view(1, 784)\n",
    "        with torch.no_grad():\n",
    "            logps = model(img)\n",
    "        \n",
    "        ps = torch.exp(logps)\n",
    "        probab = list(ps.numpy()[0])\n",
    "        \n",
    "        pred_label = probab.index(max(probab))\n",
    "        true_label = labels.numpy()[i]\n",
    "        \n",
    "        if pred_label == true_label:\n",
    "            correct_classification += 1\n",
    "        total_images += 1\n",
    "        \n",
    "print(f\"Images tested: {total_images} \\n Classification Accuracy: {correct_classification/total_images}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'mnist_base_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_model = testing_model.eval()\n",
    "final_output_layer = model[-2:].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(example_image)\n",
    "\n",
    "feature_vector = frozen_model(torch.tensor(example_image).view(1, -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
